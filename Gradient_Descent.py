"""
ГРАДИЕНТНЫЙ СПУСК
состав файла:
1. Иллюстрации градиентного спуска на примере обучения бинарного классификатора.
   Одномерное признаковое пространство.
   Программирование с нуля.
2. Тот же алгоритм, но с логарифмом рельефа функционала качества на графике.
3. Сравнение результатов градиентного спуска с точным решением
    на основе восстановления линейной регрессии методом LinearRegression() из sklearn
"""


"""
1. Иллюстрации градиентного спуска на примере обучения бинарного классификатора. 
   Одномерное признаковое пространство.
   Программирование с нуля.

"""
import matplotlib.pyplot as plt
import numpy as np
from numpy import linalg as LA

# Исходные данные
X1 = np.array([1, 2, 6, 8, 10])  # Марица значений признака объектов
# метки классов (правильные ответы)
y = np.array([-1, -1, 1, 1, 1])  # Вектор соответствующих меток классов

# Функционал качества (сумма квадратов ошибок классификации) для линейной модели y=w0+w1x
def sse_func(w0, w1,x,y):
    return ((w0+w1*x - y)**2).sum()


# Функция вектора градиента функционала качества (частные производные по параметрам модели w0 и w1)
def df_dw(W,x,y):
    return np.array([(2*(W[0]+W[1]*x - y)).sum(), (2*np.dot(x,(W[0]+W[1]*x - y)))])


# Задание начального приближения параметров модели классификатора
w00, w01 = 8, 0.8  # Начальные значения w0 и w1
# w00, w01 = -0.9, -0.9
z00 = sse_func(w00, w01, X1, y)  # Начальное значение функционала качества


# ПОСТРОЕНИЕ РЕЛЬЕФА ФУНКЦИОНАЛА КАЧЕСТВА ДЛЯ ИЛЛЮСТРАЦИИ ГРАДИЕНТНОГО СПУСКА
# Формирование координатной решетки для визуализации рельефа функционала качества
w0 = np.linspace(-10, 10, 200)  # Массив значений w0
w1 = np.linspace(-1, 1, 200)  # Массив значений w1
ww0, ww1 = np.meshgrid(w0, w1)  # Решетка значений параметров модели

# Расчет поверхности функционала качества
sse = []  # Массив для запоминания значений функционала качества
for j in range(len(w1)):
    sse.append([])
    for i in range(len(w0)):
         sse[j].append(sse_func(ww0[j][i], ww1[j][i], X1, y))
sse = np.array(sse)  # Форматирование значений рельефа

# Определение индекса глобального миниммума функционала качества на решетке
min_ind = np.unravel_index(np.argmin(sse), sse.shape)

# Индикация рельефа функционала качества в 3D
fig = plt.figure(figsize = (16, 9))
ax = plt.axes(projection='3d')
ax.plot_wireframe(ww0, ww1, sse, alpha=0.8, rstride=12, cstride=12)
ax.set_xlabel("w0")
ax.set_ylabel("w1")
ax.set_zlabel('C(X,W)')
ax.scatter(w00, w01, z00, marker='o', color='m', s=100, alpha=1)


# Градиентный спуск с идикацией последовательных приближений параметров модели
Wt = np.array([w00,w01])  # Инициализация начального приближения
ht = 0.001  # Параметр скорости градиентного спуска
dw_min = 0.0001  # Пороговое изменение параметров модели для критерия останова градиентного спуска
n = 0  # Инициализация счетсика шагов градиентного спуска
dw = ht * df_dw(Wt,X1,y)  # Расчет первого шага градиентного спуска
while LA.norm(dw) > dw_min:
    # Индикация вектора текущих параметров модели
    ax.scatter(Wt[0], Wt[1], sse_func(Wt[0], Wt[1], X1, y), marker='o', color='m', s=50, alpha=1)
    Wt = Wt - dw  # Расчет параметров модели на следующем шаге приближения
    dw = ht * df_dw(Wt,X1,y)  # Расчет величины следующего шага приближения
    n += 1
else:
    Wend = Wt  # Фиксация результата обучения модели
    # Индикация вектора параметров решения]
    ax.scatter(Wt[0], Wt[1], sse_func(Wt[0], Wt[1], X1, y), marker='o', color='red', s=50, alpha=1)

# изменение ракурса графика
ax.view_init(15, 130)

# Печать результатов обучения модели классификатора
print('Число шагов градиентного спуска',n)  # Печать числа шаго градиентного спуска
print('w* перебором по решетке значений', ww0[min_ind], ww1[min_ind])  # # Глобальный оптимум на решетке
print('w*=', Wend)  # Найденные параметры модели по итогам градиентного спуска
print('C(x,w*)=',sse_func(Wend[0], Wend[1], X1, y))  # Соответствующее решению значение функционала качества

plt.show()

"""
2. Тот же алгоритм, но с логарифмом рельефа функционала качества на графике

"""
import matplotlib.pyplot as plt
import numpy as np
from numpy import linalg as LA

# Исходные данные
X1 = np.array([1, 2, 6, 8, 10])  # Марица значений признака
# метки классов (правильные ответы)
y = np.array([-1, -1, 1, 1, 1])  # Вектор соответствующих меток классов


# Функционал качества (сумма квадратов ошибок классификации) для линейной модели y=w0+w1x
def sse_func(w0, w1, x, y):
    return ((w0 + w1 * x - y) ** 2).sum()


# Функция вектора градиента функционала качества (частные производные по параметрам модели w0 и w1)
def df_dw(W, x, y):
    return np.array([(2 * (W[0] + W[1] * x - y)).sum(), (2 * np.dot(x, (W[0] + W[1] * x - y)))])


# Задание начального приближения параметров модели классификатора
w00, w01 = 8, 0.8  # Начальные значения w0 и w1
# w00, w01 = -0.9, -0.9
z00 = sse_func(w00, w01, X1, y)  # Начальное значение функционала качества

# ПОСТРОЕНИЕ РЕЛЬЕФА ФУНКЦИОНАЛА КАЧЕСТВА ДЛЯ ИЛЛЮСТРАЦИИ ГРАДИЕНТНОГО СПУСКА
# Формирование координатной решетки для визуализации рельефа функционала качества
w0 = np.linspace(-10, 10, 100)  # Массив значений w0
w1 = np.linspace(-1, 1, 100)  # Массив значений w1
ww0, ww1 = np.meshgrid(w0, w1)  # Решетка значений параметров модели

# Расчет поверхности функционала качества
sse = []  # Массив для запоминания значений функционала качества
for j in range(len(w1)):
    sse.append([])
    for i in range(len(w0)):
        sse[j].append(sse_func(ww0[j][i], ww1[j][i], X1, y))
sse = np.array(sse)  # Форматирование значений рельефа

# Индикация рельефа функционала качества в 3D
fig = plt.figure(figsize=(16, 9))
ax = plt.axes(projection='3d')
ax.plot_wireframe(ww0, ww1, np.log(sse), alpha=0.8, rstride=6, cstride=6)
ax.set_xlabel("w0")
ax.set_ylabel("w1")
ax.set_zlabel('ln[C(X,w)]')
# ax.scatter(w00, w01, z00, marker='o', color='m', s=100, alpha=1)

# Градиентный спуск с идикацией последовательных приближений параметров модели
Wt = np.array([w00, w01])  # Инициализация начального приближения
ht = 0.001  # Параметр скорости градиентного спуска
dw_min = 0.0001  # Пороговое изменение параметров модели для критерия останова градиентного спуска
n = 0  # Инициализация счетсика шагов градиентного спуска
dw = ht * df_dw(Wt, X1, y)  # Расчет первого шага градиентного спуска
while LA.norm(dw) > dw_min:
    # Индикация вектора текущих параметров модели
    ax.scatter(Wt[0], Wt[1], np.log(sse_func(Wt[0], Wt[1], X1, y)), marker='o', color='m', s=50, alpha=0.8)
    Wt = Wt - dw  # Расчет параметров модели на следующем шаге приближения
    dw = ht * df_dw(Wt, X1, y)  # Расчет величины следующего шага приближения
    n += 1
else:
    Wend = Wt  # Фиксация результата обучения модели
    # Индикация вектора параметров решения]
    ax.scatter(Wt[0], Wt[1], np.log(sse_func(Wt[0], Wt[1], X1, y)), marker='o', color='red', s=50, alpha=1)

# изменение ракурса графика
ax.view_init(15, 130)

# Печать результатов обучения модели классификатора
print(n)  # Печать числа шаго градиентного спуска
print('w*=', Wend)  # Найденные параметры модели по итогам градиентного спуска

plt.show()



"""
3. Сравнение результатов градиентного спуска с точным решением 
   на основе восстановления линейной регрессии методом LinearRegression() из sklearn.
   Основная иллюстрация с обучающей выборкой.

"""
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.linear_model import LinearRegression

# Данные из задачи градиентного спуска (можно закоментировать, если продолжаем одну программу п.1 или п.2)
X1 = np.array([1, 2, 6, 8, 10])  # точки - признаки (одно измерение)
y = np.array([-1, -1, 1, 1, 1])  # метки классов (правильные ответы)
w00, w01 = 8, 0.8  # Начальное приближение параметров модели классификатора для градиентного спуска
Wend = [-1.1881426, 0.25889412]  # Параметры классификатора, обученного градиентным спуском


# Формирование датасета в формате DataFrame для seaborn.scatterplot
yy=['y=-1' if y[i] < 0 else 'y=+1' for i in range(len(y))]  # Вспомогательые строковые метки классов для графики seaborn
df = pd.DataFrame({'x':pd.Series(X1), 'y':pd.Series(y), 'class':pd.Series(yy)})

# Индикация обучающей выборки
fig, ax = plt.subplots()
sns.scatterplot(data = df, x = "x", y = "y", hue = "class", s = 100, edgecolors = 'black', linewidths = 2)
ax.set_xlabel('$x$')
ax.set_ylabel('$y$')
plt.xlim(0, 11)
plt.ylim(-2, 2)
plt.legend(loc='upper left', title='class')
ax.grid()

# Индикация модели линейной части классификатора g(x,w*), обученного градиентным спуском
x_min, x_max = 0.5, 10.5
def lm_func(w, x):
    return w[0] + w[1]*x
plt.plot([x_min, x_max], [lm_func(Wend, x_min), lm_func(Wend, x_max)],
        linewidth = 2, color='black', linestyle = '-', alpha=0.7)
plt.text(2.5, -0.3, s = r'$g(x,w*)$', fontsize=12, rotation=21)

# Индикация маркеров в ответах линейной части обученного классификатора
y_end = Wend[0] + Wend[1] * X1  # Ответы линейной части классификатора обученного градиентным спуском
plt.scatter(X1, y_end,  color='black', marker='s', s=25, alpha=0.7)

# Оформление решающего правила обученного классификатора на графике
arg0 = -Wend[0] / Wend[1]  # пороговое значение признака классификатора
xlist = np.linspace(0, arg0, 10)  # Горизонтальная решетка отрицательного класса
# x2list = np.linspace(arg0, 11, 10)  # Горизонтальная решетка положительного класса
ylist = [2]*10  # Вспомогательные точки для заливки области отрицательного класса
y2list = [-2]*10  # Вспомогательные точки для заливки области отрицательного класса
plt.fill_between(xlist, ylist, y2list, color = 'blue', alpha = 0.05)  # Заливка полуплоскости отрицательного класса
plt.text(1.8, 0.5, s = r'$a(x)=-1$', fontsize=12, bbox=dict(color='w'), rotation=0)
plt.text(7, -0.7, s = r'$a(x)=+1$', fontsize=12, bbox=dict(color='w'), rotation=0)

# Построение оси признака
plt.plot([0, 11], [0, 0], linewidth = 0.5, color='black', linestyle = '-', alpha=0.7)

# Индикация решающей функци обученного классификатора
plt.plot([arg0, arg0], [-2, 2], linewidth = 2, color='black', linestyle = '--', alpha=0.7)

# Печать результатов обучения классификатора градиентным спуском
print('w* =', Wend)  # Найденные параметры модели классификатора
print('Пороговое значение признака: arg[g(x,w*)=0] =', arg0)
print()

# Точное решение на основе восстановления линейной регрессии методом LinearRegression() из sklearn.
x = df[['x']].values  # Извлечение объектов из датасета
y = df['y'].to_numpy()  # Извлечение меток классов
model = LinearRegression().fit(x, y)  # Обучает модель по выборке x,y
print('R^2:', model.score(x, y))  # Печать коэффициента детерминации R**2 модели
print('w0:', model.intercept_)  # Печать коэффициента w0 регрессии
print('[w1,w2,...]:', model.coef_)  # Печать коэффициентов [w1, w2, ...] регрессии

plt.show()